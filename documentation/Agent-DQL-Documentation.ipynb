{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning With PyTorch\n",
    "\n",
    "This notebook should be used as a guide to understand the implementation of DQL algorithm, tha was written in the Agent class. It started as a basic extraction of code comments. Also, we focus here on the 'learn' function, that calculates the loss from a batch of transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note for developers:</b> Any changes to the agent's code / train function - if approved - should include changes to this notebook. PyTorch is a great framework, but it is a challenge to understand the more complex code fragments, without some wider explanation from the author. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We declare all important stuff first. Net and Memory class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from torch import autograd, optim, nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Neural Network with variable layer sizes and 2 hidden layers.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, hidden1_size)\n",
    "        self.fc2 = torch.nn.Linear(hidden1_size, hidden2_size)\n",
    "        self.fc3 = torch.nn.Linear(hidden2_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class Memory(deque):\n",
    "    \"\"\"Subclass of deque, storing transitions batches for Agent\"\"\"\n",
    "\n",
    "    def __init__(self, maxlen):\n",
    "        super().__init__(maxlen=maxlen)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize the networks and the memory (with some random stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = env.state\n",
    "actions = env.action_space\n",
    "\n",
    "input_size = len(initial_state)\n",
    "hidden1_size = 20\n",
    "hidden2_size = 10\n",
    "output_size = len(actions)\n",
    "\n",
    "q_network = Net(input_size, hidden1_size, hidden2_size, output_size)\n",
    "target_network = Net(input_size, hidden1_size, hidden2_size, output_size)\n",
    "\n",
    "memory = Memory(100)\n",
    "for _ in range(100):\n",
    "    memory.append((np.random.rand(10), 1, -5, np.random.rand(10), False))\n",
    "\n",
    "gamma = 0.9\n",
    "epsilon = 0.1\n",
    "epsilon_decay = 0.999\n",
    "epsilon_min = 0.001\n",
    "batch_size = 4\n",
    "l_rate = 0.01\n",
    "optimizer = optim.Adagrad(q_network.parameters(), lr=l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>NOTE: We register q_network.parameters() to the optimizer as we want to update them and nothing else.</b>\n",
    "\n",
    "Define a function that agent uses to generate batches from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experience_batch():\n",
    "    \"\"\"\n",
    "    Retrieves a random batch of transitions from memory and transforms it\n",
    "    to separate PyTorch Variables.\n",
    "\n",
    "    Transition is a tuple in form of:\n",
    "    (state, action, reward, next_state, terminal_state)\n",
    "    Returns:\n",
    "        exp_batch - list of Variables in given order:\n",
    "            [0] - input_state_batch\n",
    "            [1] - action_batch\n",
    "            [2] - reward_batch\n",
    "            [3] - next_state_batch\n",
    "            [4] - terminal_mask_batch\n",
    "    \"\"\"\n",
    "\n",
    "    exp_batch = [0, 0, 0, 0, 0]\n",
    "    transition_batch = random.sample(memory, batch_size)\n",
    "\n",
    "    # Float Tensors\n",
    "    for i in [0, 2, 3, 4]:\n",
    "        exp_batch[i] = Variable(torch.Tensor([x[i] for x in transition_batch]))\n",
    "\n",
    "    # Long Tensor for actions\n",
    "    exp_batch[1] = Variable(torch.LongTensor([int(x[1]) for x in transition_batch]))\n",
    "\n",
    "    return exp_batch    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training iteration\n",
    "\n",
    "Now lets break up the *_learn()* method code into smaller chunks and observe, with commentary, what happens.\n",
    "\n",
    "First, let's see the whole, working function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train():\n",
    "    if len(memory) > batch_size:\n",
    "\n",
    "        exp_batch = get_experience_batch()\n",
    "\n",
    "        input_state_batch = exp_batch[0]\n",
    "        action_batch = exp_batch[1]\n",
    "        reward_batch = exp_batch[2]\n",
    "        next_state_batch = exp_batch[3]\n",
    "        terminal_mask_batch = exp_batch[4]\n",
    "\n",
    "        all_q_values = self.q_network(input_state_batch)\n",
    "\n",
    "        q_values = all_q_values.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "        q_next_max = self.q_network(next_state_batch)\n",
    "        q_next_max = Variable(q_next_max.data)\n",
    "        q_next_max, _ = q_next_max.max(dim=1)\n",
    "\n",
    "        q_t1_max_with_terminal = q_next_max.mul(1 - terminal_mask_batch)\n",
    "\n",
    "        targets = reward_batch + self.gamma * q_t1_max_with_terminal\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = nn.modules.SmoothL1Loss()(q_values, targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets go step by step. Please note that the sizes of printed Variables are pretty important.\n",
    "\n",
    "- We sample random transition batch and transform it into separate batches (of autograd.Variable type). This is just a basic data preparation and the logic is handled by <b>get_experience_batch</b> method.\n",
    "- Make sure you understand how the data looks after this. We had transitions, but now we have separate batches. So, to retrieve the first tranisiton from the batches, you should take the first elements from each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.4393  0.3313  0.3760  0.1748  0.8239  0.8780  0.5023  0.3082  0.4402  0.3039\n",
      " 0.8808  0.4934  0.5943  0.0300  0.3950  0.3696  0.3727  0.9839  0.7707  0.0806\n",
      " 0.7093  0.3182  0.9944  0.9340  0.6316  0.4987  0.4417  0.0009  0.9415  0.6087\n",
      " 0.6867  0.4861  0.3319  0.7815  0.3298  0.1584  0.8497  0.4971  0.5337  0.5132\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.LongTensor of size 4]\n",
      "\n",
      "\n",
      " Variable containing:\n",
      "-5\n",
      "-5\n",
      "-5\n",
      "-5\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 0.8113  0.1527  0.7908  0.3781  0.1950  0.6430  0.2460  0.5829  0.4210  0.6513\n",
      " 0.1523  0.0812  0.0383  0.8162  0.2590  0.1824  0.4463  0.2484  0.4993  0.7634\n",
      " 0.7250  0.2762  0.0536  0.2136  0.4889  0.0851  0.0691  0.6098  0.4505  0.5529\n",
      " 0.0389  0.0098  0.9415  0.3856  0.3839  0.6806  0.3650  0.8824  0.2243  0.0564\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n",
      "\n",
      " Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_batch = get_experience_batch()\n",
    "\n",
    "input_state_batch = exp_batch[0]\n",
    "action_batch = exp_batch[1]\n",
    "reward_batch = exp_batch[2]\n",
    "next_state_batch = exp_batch[3]\n",
    "terminal_mask_batch = exp_batch[4]\n",
    "\n",
    "print(input_state_batch)\n",
    "print(\"\\n\", action_batch)\n",
    "print(\"\\n\", reward_batch)\n",
    "print(\"\\n\", next_state_batch)\n",
    "print(\"\\n\", terminal_mask_batch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. We start to calculate all the important values needed to compute the error used to train the network.\n",
    "\n",
    "- As Deep Q Learning states, we want to calculate the error like this: <b>Q(s,a) - (r + max{a}{Q(s_next,a)})</b>\n",
    "\n",
    "- ( This differs a bit if we use Double Deep Q Learning, so if you work on it, make sure to change it! )\n",
    "\n",
    "- So lets start with calculating the Q(s) values for each input state in input_state_batch:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1443 -0.1086 -0.2017 -0.2203\n",
      "-0.1597 -0.1120 -0.1660 -0.2246\n",
      "-0.1552 -0.1260 -0.1731 -0.2242\n",
      "-0.1414 -0.0773 -0.1912 -0.2129\n",
      "[torch.FloatTensor of size 4x4]\n",
      " torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "all_q_values = q_network(input_state_batch)\n",
    "print(all_q_values, all_q_values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Now, retrieve q_values only for actions that were taken - we want Q(s,a), not Q(s)! \n",
    "\n",
    "- The squeeze / unsqueeze functions are used because of size mismatches. See http://pytorch.org/docs/stable/torch.html#torch.squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1086\n",
      "-0.1120\n",
      "-0.1260\n",
      "-0.0773\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_values = all_q_values. \\\n",
    "                gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This use of gather (http://pytorch.org/docs/stable/torch.html#torch.gather) function works the same as a basic loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Variable containing:\n",
      "-0.1086\n",
      "[torch.FloatTensor of size 1]\n",
      ", Variable containing:\n",
      "-0.1120\n",
      "[torch.FloatTensor of size 1]\n",
      ", Variable containing:\n",
      "-0.1260\n",
      "[torch.FloatTensor of size 1]\n",
      ", Variable containing:\n",
      "1.00000e-02 *\n",
      " -7.7259\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "q_values_test = []\n",
    "for i in range(len(all_q_values)): \n",
    "    q_values_test.append(all_q_values[i][action_batch[i]])\n",
    "    \n",
    "print(q_values_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>but we cannot use such loop, because Variables are immutable, so they don't have 'append' function etc.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calculate q_next_max = max{a}{Q(s_next,a)}\n",
    "\n",
    "- Max function return the values <b>and</b> indices (http://pytorch.org/docs/stable/torch.html#torch.max), but we just want the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1351 -0.1182 -0.1916 -0.2275\n",
      "-0.1100 -0.0754 -0.2378 -0.2160\n",
      "-0.1158 -0.0998 -0.2051 -0.2263\n",
      "-0.1435 -0.1067 -0.1973 -0.2218\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "Variable containing:\n",
      "-0.1182\n",
      "-0.0754\n",
      "-0.0998\n",
      "-0.1067\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_next_max = q_network(next_state_batch)\n",
    "q_next_max = Variable(q_next_max.data)\n",
    "print(q_next_max)\n",
    "q_next_max, _ = q_next_max.max(dim=1)\n",
    "print(q_next_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So as we see - we calculate the outputs and take the biggest ones\n",
    "- Note: We create new Variable after the first line. Why?\n",
    "- We used the q_network parameters to calculate q_next_max, but we don't want the backward() function to propagate twice into these parameters. Creating new Variable 'cuts' this part of computational graph - prevents it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. If the next state was terminal, we don't calculate the q value the target should be just = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1182\n",
      "-0.0754\n",
      "-0.0998\n",
      "-0.1067\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_t1_max_with_terminal = q_next_max.mul(1 - terminal_mask_batch)\n",
    "print(q_t1_max_with_terminal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So if the next state would be terminal, there would be zeros in corresponding places!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate the target = r + max{a}{Q(s_next,a)}\n",
    "\n",
    "- We have the rewards and the max Q values for next states, so we can calculate the target by adding them up.\n",
    "- Of course, we multiply by gamma here, as stated in the Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-5.1064\n",
      "-5.0678\n",
      "-5.0898\n",
      "-5.0960\n",
      "[torch.FloatTensor of size 4]\n",
      "\n",
      "Variable containing:\n",
      "-0.1086\n",
      "-0.1120\n",
      "-0.1260\n",
      "-0.0773\n",
      "[torch.FloatTensor of size 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "targets = reward_batch + gamma * q_t1_max_with_terminal\n",
    "\n",
    "print(targets)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, what we see? We have the networks predictions, and the target values. Nicely trained agent (with reasonable environment transitions) makes the predicitions close to the targets. Our agent does a bad job.\n",
    "\n",
    "### Loss function and backward()\n",
    "\n",
    "- To train him into a better direction, we now calculate the loss function (yes, we usually don't use the raw error and calculate a loss function):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.4841\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss = nn.modules.SmoothL1Loss()(q_values, targets)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We make use of pytorch's optimizer. We have to zero the gradients before calculating new ones.\n",
    "\n",
    "- We calculate the loss using built-in SmoothL1Loss, called Huber Loss, which is a recommended choice for Q-Learning\n",
    "\n",
    "- We call the <b>backward</b> function, which traces all values that were used in the process of calculating the loss. The most important thing here, is that it will calculate the gradients for all q_network weights and biases.\n",
    "\n",
    "- We calculated the gradients, but the update happens only with <b>optimizer.step()</b> call. It takes  the gradients and updates the parameters (just the parameters that were registered to the optimizer!), finalizing the training iteration. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-dec",
   "language": "python",
   "name": "rl-dec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
