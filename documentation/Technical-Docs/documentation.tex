\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{amsmath} % Required for some math elements
\usepackage{graphicx} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{fullpage}

% \setlength\parindent{0pt} % Removes all indentation from paragraphs

\title{Effective House Energy Management Using Reinforcement Learning Technical Documentation} % Title

\author{Dawid Czarneta, Jakub Frąckiewicz, Filip Olszewski, Michał Popiel, Julia Szulc}

\date{\today}

\begin{document}

\maketitle % Insert the title, author and date

\begin{abstract}
This is a technical documentation of our university project developed with cooperation and
guidance of Rafał Pilarczyk from SAMSUNG. Document contains a short introduction and overview of main concepts, milestones, goals and effects of this project. It is followed by a technical part, that contains a setup guide, code and tests overviews, explanation of used algorithms and concepts (including the Reinforcement Learning part) and details about the development process, providing useful insights, experiences and lessons we have learned the hard way.
\end{abstract}

\section{Introduction}
% wstęp, idea, kontekst
\subsection{Goals and milestones}
% jak w nazwie
\subsection{Results}
% jak w nazwie

\section{Reinforcement Learning}
The project is based on one of the machine learning areas called reinforcement learning. The research and development of this field has been rising as of late, because of, among others, the improvements in deep learning methods and constantly increasing computing power. Combined with concepts like convolutional neural networks, RL algorithms  achieve super-human performances in a variety of tasks. The main field of success is probably the gaming world, including both the traditional games like Chess or Go, and computer games, where, for example, classic Atari games are learned and solved by RL agents taking the input directly from raw pixels.

Reinforcement learning is often explained by comparison to supervised learning, which is usually named the most successful field of Machine Learning. It is based on datasets, which consist of both the observations and the correct labels (answers, classes). For example, the dataset consists of images, and each image has it is correct label as well - a word that describes the object on the image. In Reinforcement Learning, on the other hand, there is no 'supervisor'. Instead, the environment returns a reward signal, which indicates how good or bad the current situation is. The main goal of the agent is to maximize the cummulative reward.

There is a common vocabulary that is used to describe this setup. The \textbf{agent} (RL algorithm) has to interact with the \textbf{environment}, by performing \textbf{actions}. Environment gives the agent information about the \textbf{current state}, as well as the \textbf{reward signal} for the last timeframe. Agent has to choose the best one from the given set of possible actions. This is usually happening until the environment reaches a \textbf{terminal state}. Simulation from the start to terminal state is called an \textbf{episode}. A \textbf{Transition} is defined as a sequence - state, action, reward and next state - and is used as a minimal

To put this into perspective: In our project, the smart house manager is the agent. The simulated world is the environment. Actions are used to control devices such as heating or light. The state is a collection of information about the weather, inside parameters, current devices settings and user requests - everything that is needed for agent to perform rational decisions. The episode can be defined as - for example - one single day, where the midnight is a terminal state.
\subsection{What have we used?}
Our agent uses Double DQN with Prioritized Experience Replay. This is a combination of recent developments in the field, which improve the speed, quality and stability of learning, and sometimes are essential for solving particular tasks. For an explanation of these concepts and more detailed information about the agent, please see the Agent module section (4.2).
\subsection{PyTorch Framework}
To implement the agent and the concepts named above, we have used the PyTorch framework. It is now commonly believed, that this frameworks becomes the go-to Python framework used for Deep Learning, with some essential advantages when compared to TensorFlow. While this is still a matter of loud discusions, we have chosen the framework more out of curiosity, and because it is often called 'more Pythonic'.

We are using PyTorch for a very clear neural network model declaration, ready-to-use optimizers like SGD or Adagrad, automatic computation of the gradients of network's parameters, and to perform the Double DQN learning process. It also provides a clear way to use GPUs for the computations. 

We have used the version 0.3.1 and it is important to note, that the newest version of the framework (0.4.0 on the day of this documentation's publication) requires some changes to the code.
\section{Getting Started}
\subsection{Setup and Requirements}
% Co zainstalować, jaka wersja Pythona, wszyściutko
\subsection{Learning and Simulation}
% Tutaj opis możliwych trybów uruchomienia. Main/learning na start, potem symulacja. Zaznaczyć już tutaj istotność configuration.json itd.
\subsection{Testing}
% Opis testowania projektu. Testy jednostkowe wraz ze sposobem generowania coverage.
% Do tego manual testing - testowanie środowiska (może być jako subsubsection, a może być jako osobne subsection)
\section{Code overview}
% wstępniaczek co znajdziemy w tej sekcji.
\subsection{HouseEnergyEnvironment module}
% opis środowiska. Tutaj chyba najlepiej podzielić to na subsubsections, House / World / Environment

\subsection{Agent module}
% tutaj ogarnę jeszcze jak to podzielę, bo jeszcze nie wiem // filip

\subsection{Configuration file}
% Tutaj ładnie wylistowane parametry z dosyć dokładnym wytłumaczeniem - czego dotyczą, gdzie są używane, poziom istotności parametru
% na końcu może być wrzucona obecna konfiguracja z jakąś adnotacją, że dla tej konfiguracji generowaliśmy/uczyliśmy ostateczny model agenta

\section{Accuracy measures and tempo of learning}
% tutaj trochę o statystykach które pokazujemy w mainie, jakie wyniki uznajemy za agenta nauczonego, jaki procent odpalonych agentów uczy się wszystkiego, ile średnio epizodów zajmuje osiągnięcie akceptowalnego poziomu. Chciałoby się, żeby ta sekcja była zrobiona bardzo obszernie, ale nie mamy czasu żeby przeprowadzić nie wiadomo jaką ilość testów - i o tym też wspomnijmy. 

% Uwaga - Tutaj powinna znaleźć się subsekcja o wpływie parametrów z configuraiton.json na uczenie - głównie tych od strony agenta, ale też nie mamy danych żeby tworzyć tutaj jakieś pewniki i pisać tutaj, że 'na 100%' większy batch size daje gorsze wyniki itd. Z umiarem :p
\section{Development Process}
% wstępniak - z racji, że to projekt badawczy a nie gotowy produkt, to zamieszczamy tutaj informacje o tym, jak szły prace, co jest trudne, co nie poszło, co poszło superancko itd.
\subsection{Chronology}
% Rafał sugerował coś takiego, myślę że nie ma co wypisywać tasków po kolei od lutego, ale przydałoby się zrobić listę mniej więcej jak to po kolei powstawało. Być może osobne dla enva i agenta.
\subsection{What Failed}
% zmienić nazwę tej sekcji na jakąś ładniejszą!
% które założenia 'podupadły', które rozwiązania okazały się nieskuteczne / za trudne itd.
% zarówno rzeczy typu Sparse Rewards (koncepcja zawiodła), jak i skalowalność projektu (nie jest zapewniona idealnie, my zawiedliśmy z braku enterpris'owego doświadczenia)

\section{Conclusions}
% tutaj zarówno wnioski mocno badawcze o RL, jak i o rozwoju projektu. Myślę, że potencjalne kierunki rozwoju projektu należy wpisać jako subsekcję tej sekcji.

\section{Bibliography and Useful Sources}
% papery do użytych konceptów, źródła z których się uczyliśmy, linki do bibliotek i te de
\end{document}