\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{amsmath} % Required for some math elements
\usepackage{graphicx} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[bottom]{footmisc}
\usepackage{listings}
\lstset{basicstyle=\fontfamily{fvm}\selectfont\footnotesize,
    showstringspaces=false,
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}

% \setlength\parindent{0pt} % Removes all indentation from paragraphs

\title{Effective House Energy Management Using Reinforcement Learning Technical Documentation} % Title

\author{Dawid Czarneta, Jakub Frąckiewicz, Filip Olszewski, Michał Popiel, Julia Szulc}

\date{\today}

\begin{document}

\maketitle % Insert the title, author and date

\begin{abstract}
This is a technical documentation of our university project developed with cooperation and
guidance of Rafał Pilarczyk from SAMSUNG. Document contains a short introduction and overview of main concepts, milestones, goals and effects of this project. It is followed by a technical part, that contains a setup guide, code and tests overviews, explanation of used algorithms and concepts (including the Reinforcement Learning part) and details about the development process, providing useful insights, experiences and lessons we have learned the hard way.
\end{abstract}

\section{Introduction}
% wstęp, idea, kontekst

\subsection{Goals and milestones}
% jak w nazwie
\subsection{Results}
% jak w nazwie

\section{Reinforcement Learning}
% co to, gdzie sukcesy, czym się różni od dotychczasowych rozwiązań
\subsection{What have we used?}
% opis rozwiązań użytych w obecnym agencie. Q/Target split, Double DQN, PER, reward clipping
% architektura sieci (tylko ile warstw), więcej o sieci jeszcze będzie w sekcji agenta
\subsection{PyTorch Framework}
% kontekst, dlaczego go użyliśmy, do czego go użyliśmy - więcej w sekcji o Agencie
\section{Getting Started}
\subsection{Setup and Requirements}
% Co zainstalować, jaka wersja Pythona, wszyściutko
This project was made on Debian based Linux distributions. We strongly 
recommend it in similar environment. 
\\\\
Firstly, install Python 3.6.5. You can do it via apt package manager from command line
\begin{lstlisting}[language=bash]
$ sudo apt-get install python3.6 
\end{lstlisting}
or by downloading it from producent's page
\footnote{\url{https://www.python.org/downloads/release/python-365/}}
 and performing manual installation.
\\ \\ 
Secondly, go to main project directory and install required Python libraries:
\begin{lstlisting}[language=bash]
$ pip3 install -r requirements.txt
\end{lstlisting}
From now on, you should be able to use our project by running different source scripts.

\subsection{Usage: Learning and Simulation}
% Tutaj opis możliwych trybów uruchomienia. Main/learning na start, potem symulacja. Zaznaczyć już tutaj istotność configuration.json itd.
Our project can be run in different modes. These can be configured by changing contents of 
\textbf{configuration.json} file, located in main project directory. This directory will be referred to as 
\textbf{TOP} in this section.

\subsubsection{Learning mode}
This mode can be run by going into source files directory and launching \textbf{main.py}
 with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 main.py
\end{lstlisting}
In learning mode, Reinforcement Learning Agent model is learning for a given number of episodes.
One episode is typically one day in randomly generated environment. Agent is performing action every few minutes, trying to get perfect balance between required conditions (light, temperature) and energy cost; he is also drawing conslusions from his mistakes. As the time passes, Agent is getting better; we can roughly say
that in the long term, a longer trained Agent will perform better than the one after shorter training session,
with the same training configuration.
\\\\
After training session, model is typically saved in
\begin{lstlisting}
$ TOP/src/saved_models/
\end{lstlisting}
directory, within a separate folder with new index (indexes start at 0). Model is saved along with its configuration file, graph presenting learning curve and log with rewards from 
every step.
\\\\
Existing agent model can also be loaded into training session. In this case, you will be prompted
to insert existing model's index number.
\\\\ Basic configuration for this mode can be modified by changing contents of configuration.json, a part
called \textbf{main}. Precise documentation of the whole configuration file can be found later in documentation. 
\\\\
\textit{NOTE:} Altough basic configuration for main (learning) mode is fairly limited, succesfull learning is built upon all environment / agent settings in configuration file. Default ones are based on our research in order to provide the best and most meaningful learning results.

\subsubsection{Simulation mode}
This mode can be run by going into source files directory and launching \textbf{simulation.py}
 with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 simulation.py
\end{lstlisting}
After launching, you will be prompted to insert existing model's number to use in simulation. 
\\\\
\textit{NOTE:} Simulation will start in full screen, be sure to have only one monitor connected - otherwise, simulation will span across all monitors.
\\\\
In simulation mode, user is presented to graphical interface representing Reinforcement Learning
Agent in action. 
There is a continuous environment, inside which Agent is trying to make his best decisions. No
learning process included.  \\\\
\textbf{Key mapping scheme}: \\
\begin{center}
\begin{tabular}{c | c }
\textbf{key} & \textbf{action} \\ \hline
ESC & exit simulation \\ \hline
SPACEBAR & toggle pause / play \\ \hline
- & slow down simulation \\ \hline
= & speed up simulation \\ \hline
z & toggle charts zoom \\ 
\end{tabular}
\end{center}
\textbf{Widgets}
\begin{itemize}
    \item Weather widget \\\\
    Located on the left side of the screen. This widget presents current time and weather informations outside of the house in form of a timer animation and five weather indicators: 
    \begin{itemize}
        \item temperature
        \item sun intensity
        \item wind intensity
        \item clouds intensity
        \item rain intensity
    \end{itemize}
    \item Devices widget \\\\
    Located on the right bottom of the screen. This widget presents current house devices settings. Those include:
    \begin{itemize}
       \item Energy source (icons) and battery level (percentage)
       \item Cooling, heating, light and curtains level (leveled bar chart) 
    \end{itemize}
    \item Charts widget \\\\
    Located on the right center of the screen. This widget presents last 100 levels of light / degrees of temperature inside the house, along with users required level at given time. \\\\
    These two charts can be zoomed by pressing ''z'' on the keyboard. Zooming centers chart in a way that required level is placed directly at half of the charts height and the visible range is between $-0.1 * max$ and $+0.1 * max$ for maximum light / temperature levels respectively.
    \item Gauges widget \\\\
    Located on the right top of the screen. This widget presents current and desired level of light / temperature in form of a gauge chart.
\end{itemize}

Default speed of simulation can be set in \textbf{configuration.json} file, in module ''simulation'', item ''fps''.
\subsubsection{Manual testing mode}
This mode can be run by going into source files directory and launching \textbf{manual\_test.py} with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 manual_test.py
\end{lstlisting}
This mode is a command-line interface for manual testing of Reinforcement Learning Agent behaviour. User can manually test agent-environment connection by deciding which action to make, or can load existing model to be able to use its choice in particular moment. The whole process is done one step at the time.\\\\
All actions are made by entering a number specific for a certain command. Commands and numbers are visible all the time on top of the rendered text. \\\\
\textit{NOTE:} This mode was made mainly for testing purposes, and should be treated as additional, not main functionality.

\subsection{Testing}
% Opis testowania projektu. Testy jednostkowe wraz ze sposobem generowania coverage.
% Do tego manual testing - testowanie środowiska (może być jako subsubsection, a może być jako osobne subsection)
\section{Code overview}
% wstępniaczek co znajdziemy w tej sekcji.
\subsection{HouseEnergyEnvironment module}
% opis środowiska. Tutaj chyba najlepiej podzielić to na subsubsections, House / World / Environment

\subsection{Agent module}
% tutaj ogarnę jeszcze jak to podzielę, bo jeszcze nie wiem // filip

\subsection{Configuration file}
% Tutaj ładnie wylistowane parametry z dosyć dokładnym wytłumaczeniem - czego dotyczą, gdzie są używane, poziom istotności parametru
% na końcu może być wrzucona obecna konfiguracja z jakąś adnotacją, że dla tej konfiguracji generowaliśmy/uczyliśmy ostateczny model agenta

\section{Accuracy measures and tempo of learning}
% tutaj trochę o statystykach które pokazujemy w mainie, jakie wyniki uznajemy za agenta nauczonego, jaki procent odpalonych agentów uczy się wszystkiego, ile średnio epizodów zajmuje osiągnięcie akceptowalnego poziomu. Chciałoby się, żeby ta sekcja była zrobiona bardzo obszernie, ale nie mamy czasu żeby przeprowadzić nie wiadomo jaką ilość testów - i o tym też wspomnijmy. 

% Uwaga - Tutaj powinna znaleźć się subsekcja o wpływie parametrów z configuraiton.json na uczenie - głównie tych od strony agenta, ale też nie mamy danych żeby tworzyć tutaj jakieś pewniki i pisać tutaj, że 'na 100%' większy batch size daje gorsze wyniki itd. Z umiarem :p
\section{Development Process}
% wstępniak - z racji, że to projekt badawczy a nie gotowy produkt, to zamieszczamy tutaj informacje o tym, jak szły prace, co jest trudne, co nie poszło, co poszło superancko itd.
\subsection{Chronology}
% Rafał sugerował coś takiego, myślę że nie ma co wypisywać tasków po kolei od lutego, ale przydałoby się zrobić listę mniej więcej jak to po kolei powstawało. Być może osobne dla enva i agenta.
\subsection{What Failed}
% zmienić nazwę tej sekcji na jakąś ładniejszą!
% które założenia 'podupadły', które rozwiązania okazały się nieskuteczne / za trudne itd.
% zarówno rzeczy typu Sparse Rewards (koncepcja zawiodła), jak i skalowalność projektu (nie jest zapewniona idealnie, my zawiedliśmy z braku enterpris'owego doświadczenia)

\section{Conclusions}
% tutaj zarówno wnioski mocno badawcze o RL, jak i o rozwoju projektu. Myślę, że potencjalne kierunki rozwoju projektu należy wpisać jako subsekcję tej sekcji.

\section{Bibliography and Useful Sources}
% papery do użytych konceptów, źródła z których się uczyliśmy, linki do bibliotek i te de
\end{document}