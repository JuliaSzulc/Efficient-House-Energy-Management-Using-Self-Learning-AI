\documentclass{article}

\usepackage{graphicx} % Required for the inclusion of images
\usepackage{amsmath} % Required for some math elements
\usepackage{graphicx} 
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{fullpage}
\usepackage{xcolor}
\usepackage[hidelinks]{hyperref}
\usepackage[bottom]{footmisc}
\usepackage{listings}
\lstset{basicstyle=\fontfamily{fvm}\selectfont\footnotesize,
    showstringspaces=false,
    commentstyle=\color{red},
    keywordstyle=\color{blue}
}

% \setlength\parindent{0pt} % Removes all indentation from paragraphs

\title{Effective House Energy Management Using Reinforcement Learning Technical Documentation} % Title

\author{Dawid Czarneta, Jakub Frąckiewicz, Filip Olszewski, Michał Popiel, Julia Szulc}

\date{\today}

\begin{document}

\maketitle % Insert the title, author and date

\begin{abstract}
This is a technical documentation of our university project developed with cooperation and
guidance of Rafał Pilarczyk from SAMSUNG. Document contains a short introduction and overview of main concepts, milestones, goals and effects of this project. It is followed by a technical part, that contains a setup guide, code and tests overviews, explanation of used algorithms and concepts (including the Reinforcement Learning part) and details about the development process, providing useful insights, experiences and lessons we have learned the hard way.
\end{abstract}

\section{Introduction}
% wstęp, idea, kontekst

\subsection{Goals and milestones}
% jak w nazwie
\subsection{Results}
% jak w nazwie

\section{Reinforcement Learning}
The project is based on one of the machine learning areas called reinforcement learning. The research and development of this field has been rising as of late, because of, among others, the improvements in deep learning methods and constantly increasing computing power. Combined with concepts like convolutional neural networks, RL algorithms  achieve super-human performances in a variety of tasks. The main field of success is probably the gaming world, including both the traditional games like Chess or Go, and computer games, where, for example, classic Atari games are learned and solved by RL agents taking the input directly from raw pixels.

Reinforcement learning is often explained by comparison to supervised learning, which is usually named the most successful field of Machine Learning. It is based on datasets, which consist of both the observations and the correct labels (answers, classes). For example, the dataset consists of images, and each image has it is correct label as well - a word that describes the object on the image. In Reinforcement Learning, on the other hand, there is no 'supervisor'. Instead, the environment returns a reward signal, which indicates how good or bad the current situation is. The main goal of the agent is to maximize the cummulative reward.

There is a common vocabulary that is used to describe this setup. The \textbf{agent} (RL algorithm) has to interact with the \textbf{environment}, by performing \textbf{actions}. Environment gives the agent information about the \textbf{current state}, as well as the \textbf{reward signal} for the last timeframe. Agent has to choose the best one from the given set of possible actions. This is usually happening until the environment reaches a \textbf{terminal state}. Simulation from the start to terminal state is called an \textbf{episode}. A \textbf{Transition} is defined as a sequence - state, action, reward and next state - and is used as a minimal

To put this into perspective: In our project, the smart house manager is the agent. The simulated world is the environment. Actions are used to control devices such as heating or light. The state is a collection of information about the weather, inside parameters, current devices settings and user requests - everything that is needed for agent to perform rational decisions. The episode can be defined as - for example - one single day, where the midnight is a terminal state.
\subsection{What have we used?}
Our agent uses Double DQN with Prioritized Experience Replay. This is a combination of recent developments in the field, which improve the speed, quality and stability of learning, and sometimes are essential for solving particular tasks. For an explanation of these concepts and more detailed information about the agent, please see the Agent module section (4.2).
\subsection{PyTorch Framework}
To implement the agent and the concepts named above, we have used the PyTorch framework. It is now commonly believed, that this frameworks becomes the go-to Python framework used for Deep Learning, with some essential advantages when compared to TensorFlow. While this is still a matter of loud discusions, we have chosen the framework more out of curiosity, and because it is often called 'more Pythonic'.

We are using PyTorch for a very clear neural network model declaration, ready-to-use optimizers like SGD or Adagrad, automatic computation of the gradients of network's parameters, and to perform the Double DQN learning process. It also provides a clear way to use GPUs for the computations. 

We have used the version 0.3.1 and it is important to note, that the newest version of the framework (0.4.0 on the day of this documentation's publication) requires some changes to the code.
\section{Getting Started}
\subsection{Setup and Requirements}
% Co zainstalować, jaka wersja Pythona, wszyściutko
This project was made on Debian based Linux distributions. We strongly 
recommend it in similar environment. 
\\\\
Firstly, install Python 3.6.5. You can do it via apt package manager from command line
\begin{lstlisting}[language=bash]
$ sudo apt-get install python3.6 
\end{lstlisting}
or by downloading it from producent's page
\footnote{\url{https://www.python.org/downloads/release/python-365/}}
 and performing manual installation.
\\ \\ 
Secondly, go to main project directory and install required Python libraries:
\begin{lstlisting}[language=bash]
$ pip3 install -r requirements.txt
\end{lstlisting}
From now on, you should be able to use our project by running different source scripts.

\subsection{Usage: Learning and Simulation}
% Tutaj opis możliwych trybów uruchomienia. Main/learning na start, potem symulacja. Zaznaczyć już tutaj istotność configuration.json itd.
Our project can be run in different modes. These can be configured by changing contents of 
\textbf{configuration.json} file, located in main project directory. This directory will be referred to as 
\textbf{TOP} in this section.

\subsubsection{Learning mode}
This mode can be run by going into source files directory and launching \textbf{main.py}
 with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 main.py
\end{lstlisting}
In learning mode, Reinforcement Learning Agent model is learning for a given number of episodes.
One episode is typically one day in randomly generated environment. Agent is performing action every few minutes, trying to get perfect balance between required conditions (light, temperature) and energy cost; he is also drawing conslusions from his mistakes. As the time passes, Agent is getting better; we can roughly say
that in the long term, a longer trained Agent will perform better than the one after shorter training session,
with the same training configuration.
\\\\
After training session, model is typically saved in
\begin{lstlisting}
$ TOP/src/saved_models/
\end{lstlisting}
directory, within a separate folder with new index (indexes start at 0). Model is saved along with its configuration file, graph presenting learning curve and log with rewards from 
every step.
\\\\
Existing agent model can also be loaded into training session. In this case, you will be prompted
to insert existing model's index number.
\\\\ Basic configuration for this mode can be modified by changing contents of configuration.json, a part
called \textbf{main}. Precise documentation of the whole configuration file can be found later in documentation. 
\\\\
\textit{NOTE:} Altough basic configuration for main (learning) mode is fairly limited, succesfull learning is built upon all environment / agent settings in configuration file. Default ones are based on our research in order to provide the best and most meaningful learning results.

\subsubsection{Simulation mode}
This mode can be run by going into source files directory and launching \textbf{simulation.py}
 with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 simulation.py
\end{lstlisting}
After launching, you will be prompted to insert existing model's number to use in simulation. 
\\\\
\textit{NOTE:} Simulation will start in full screen, be sure to have only one monitor connected - otherwise, simulation will span across all monitors.
\\\\
In simulation mode, user is presented to graphical interface representing Reinforcement Learning
Agent in action. 
There is a continuous environment, inside which Agent is trying to make his best decisions. No
learning process included.  \\\\
\textbf{Key mapping scheme}: \\
\begin{center}
\begin{tabular}{c | c }
\textbf{key} & \textbf{action} \\ \hline
ESC & exit simulation \\ \hline
SPACEBAR & toggle pause / play \\ \hline
- & slow down simulation \\ \hline
= & speed up simulation \\ \hline
z & toggle charts zoom \\ 
\end{tabular}
\end{center}
\textbf{Widgets}
\begin{itemize}
    \item Weather widget \\\\
    Located on the left side of the screen. This widget presents current time and weather informations outside of the house in form of a timer animation and five weather indicators: 
    \begin{itemize}
        \item temperature
        \item sun intensity
        \item wind intensity
        \item clouds intensity
        \item rain intensity
    \end{itemize}
    \item Devices widget \\\\
    Located on the right bottom of the screen. This widget presents current house devices settings. Those include:
    \begin{itemize}
       \item Energy source (icons) and battery level (percentage)
       \item Cooling, heating, light and curtains level (leveled bar chart) 
    \end{itemize}
    \item Charts widget \\\\
    Located on the right center of the screen. This widget presents last 100 levels of light / degrees of temperature inside the house, along with users required level at given time. \\\\
    These two charts can be zoomed by pressing ''z'' on the keyboard. Zooming centers chart in a way that required level is placed directly at half of the charts height and the visible range is between $-0.1 * max$ and $+0.1 * max$ for maximum light / temperature levels respectively.
    \item Gauges widget \\\\
    Located on the right top of the screen. This widget presents current and desired level of light / temperature in form of a gauge chart.
\end{itemize}

Default speed of simulation can be set in \textbf{configuration.json} file, in module ''simulation'', item ''fps''.
\subsubsection{Manual testing mode}
This mode can be run by going into source files directory and launching \textbf{manual\_test.py} with python3 interpreter. 
\begin{lstlisting}
$ cd TOP/src
$ python3 manual_test.py
\end{lstlisting}
This mode is a command-line interface for manual testing of Reinforcement Learning Agent behaviour. User can manually test agent-environment connection by deciding which action to make, or can load existing model to be able to use its choice in particular moment. The whole process is done one step at the time.\\\\
All actions are made by entering a number specific for a certain command. Commands and numbers are visible all the time on top of the rendered text. \\\\
\textit{NOTE:} This mode was made mainly for testing purposes, and should be treated as additional, not main functionality.

\subsection{Testing}
% Opis testowania projektu. Testy jednostkowe wraz ze sposobem generowania coverage.
% Do tego manual testing - testowanie środowiska (może być jako subsubsection, a może być jako osobne subsection)
\section{Code overview}
% wstępniaczek co znajdziemy w tej sekcji.
\subsection{HouseEnergyEnvironment module}
% opis środowiska. Tutaj chyba najlepiej podzielić to na subsubsections, House / World / Environment

\subsection{Agent module}
% tutaj ogarnę jeszcze jak to podzielę, bo jeszcze nie wiem // filip

\subsection{Configuration file}
% Tutaj ładnie wylistowane parametry z dosyć dokładnym wytłumaczeniem - czego dotyczą, gdzie są używane, poziom istotności parametru
% na końcu może być wrzucona obecna konfiguracja z jakąś adnotacją, że dla tej konfiguracji generowaliśmy/uczyliśmy ostateczny model agenta

\section{Accuracy measures and tempo of learning}
% tutaj trochę o statystykach które pokazujemy w mainie, jakie wyniki uznajemy za agenta nauczonego, jaki procent odpalonych agentów uczy się wszystkiego, ile średnio epizodów zajmuje osiągnięcie akceptowalnego poziomu. Chciałoby się, żeby ta sekcja była zrobiona bardzo obszernie, ale nie mamy czasu żeby przeprowadzić nie wiadomo jaką ilość testów - i o tym też wspomnijmy. 

% Uwaga - Tutaj powinna znaleźć się subsekcja o wpływie parametrów z configuraiton.json na uczenie - głównie tych od strony agenta, ale też nie mamy danych żeby tworzyć tutaj jakieś pewniki i pisać tutaj, że 'na 100%' większy batch size daje gorsze wyniki itd. Z umiarem :p
\section{Development Process}
% wstępniak - z racji, że to projekt badawczy a nie gotowy produkt, to zamieszczamy tutaj informacje o tym, jak szły prace, co jest trudne, co nie poszło, co poszło superancko itd.
\subsection{Chronology}
% Rafał sugerował coś takiego, myślę że nie ma co wypisywać tasków po kolei od lutego, ale przydałoby się zrobić listę mniej więcej jak to po kolei powstawało. Być może osobne dla enva i agenta.
\subsection{What Failed}
% zmienić nazwę tej sekcji na jakąś ładniejszą!
% które założenia 'podupadły', które rozwiązania okazały się nieskuteczne / za trudne itd.
% zarówno rzeczy typu Sparse Rewards (koncepcja zawiodła), jak i skalowalność projektu (nie jest zapewniona idealnie, my zawiedliśmy z braku enterpris'owego doświadczenia)

\section{Conclusions}
% tutaj zarówno wnioski mocno badawcze o RL, jak i o rozwoju projektu. Myślę, że potencjalne kierunki rozwoju projektu należy wpisać jako subsekcję tej sekcji.

\section{Bibliography and Useful Sources}
% papery do użytych konceptów, źródła z których się uczyliśmy, linki do bibliotek i te de
\end{document}